
# ğŸ§  LLM Response Classification using Sentence Embeddings

### ğŸ“Œ Overview

This project focuses on evaluating and classifying responses generated by **Large Language Models (LLMs)**. The goal is to predict which modelâ€™s output (Model A or Model B) is preferred, or if their responses are equally good (*tie*).

To achieve this, the project uses **semantic embeddings** from the **BGE-Large model** to represent text numerically and trains a **machine learning classifier** to predict probabilities for each possible outcome.

---

### ğŸš€ Features

* Uses **BAAI/BGE-Large-EN**, a state-of-the-art embedding model for sentence similarity
* Predicts **three-class probabilities**:

  * `winner_model_a`
  * `winner_model_b`
  * `winner_tie`
* Works **offline** using a zipped version of the embedding model
* Fully compatible with **Kaggle** environments
* Optimized for speed and memory usage

---

### âš™ï¸ Tech Stack

* **Python**
* **SentenceTransformers (BGE-Large)**
* **scikit-learn** (Logistic Regression / XGBoost)
* **NumPy, pandas**
* **Matplotlib / t-SNE** for visualization
* **Jupyter Notebook / Kaggle Kernel**

---

### ğŸ§© Workflow

1. **Data Preprocessing:**

   * Load text pairs and labels from Kaggle dataset.
   * Clean and tokenize responses for embedding generation.

2. **Embedding Generation:**

   * Encode each response using the **BGE-Large** model.
   * Save embeddings locally (`.npy`) to avoid recomputation.

3. **Model Training:**

   * Train a **Logistic Regression** or **XGBoost** classifier on the embeddings.
   * Use **Stratified Sampling** to handle class imbalance.

4. **Prediction & Submission:**

   * Predict probabilities for each target class using `predict_proba()`.
   * Format the submission file as:

     ```
     id,winner_model_a,winner_model_b,winner_tie
     136060,0.33,0.33,0.33
     211333,0.80,0.10,0.10
     ...
     ```
   * Save as `submission.csv` and upload to Kaggle.

---

### ğŸ“Š Example Output

```
id,winner_model_a,winner_model_b,winner_tie
136060,0.33,0.33,0.33
211333,0.72,0.10,0.18
1233961,0.25,0.60,0.15
```

---

### ğŸ§  Challenges & Solutions

| Challenge                      | Solution                                       |
| ------------------------------ | ---------------------------------------------- |
| Large embedding model (1.3 GB) | Created offline zipped version to load quickly |
| Kaggle kernel timeouts         | Saved intermediate embeddings using `.npy`     |
| Class imbalance                | Used class weights and stratified splits       |
| Interpretability               | Visualized embeddings using t-SNE plots        |

---

### ğŸ Results

* Achieved strong leaderboard performance on Kaggle
* Efficient offline model reuse
* Improved understanding of **embedding-based evaluation** for LLM responses

---

### ğŸ§° Installation

```bash
git clone https://github.com/yourusername/llm-response-classification.git
cd llm-response-classification
pip install -r requirements.txt
```

To use offline:

```python
model = SentenceTransformer("/kaggle/input/bge-large-en-offline")
```

---

### ğŸ“¦ Requirements

```
sentence-transformers
scikit-learn
numpy
pandas
matplotlib
```

---

### ğŸ‘¨â€ğŸ’» Author

**Preetham Kothakonda**

* Electronics and Instrumentation
* NIT Rourkela
* Passionate about AI, NLP, and Machine Learning Applications

---
